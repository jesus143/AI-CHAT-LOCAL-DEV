# RAG AI Chat Application - Project Summary

## ğŸ“‹ Executive Summary

A production-ready **Retrieval-Augmented Generation (RAG)** application that enables users to upload documents and ask intelligent questions about their content. The system combines document processing, vector similarity search, and local AI to provide accurate, context-aware responses based on uploaded materials.

**Live Demo Available Upon Request**

---

## ğŸ¯ Project Overview

This application demonstrates full-stack development skills, AI/ML integration, and practical implementation of modern RAG architecture. Users can upload documents (PDF, DOCX, TXT), which are automatically processed, chunked, embedded, and indexed for semantic search. Questions asked through the chat interface retrieve relevant context from documents before generating AI responses.

### Key Capabilities
- Document upload and processing (multiple formats)
- Semantic search using vector embeddings
- **Selective file querying** - Choose which documents to search per question
- Context-aware AI responses powered by local LLM with source attribution
- Real-time chat interface with WebSocket
- Document management with interactive file selection UI
- Statistics tracking and file metadata display

---

## ğŸ—ï¸ Technical Architecture

### System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      WebSocket      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Node.js    â”‚
â”‚   (Browser) â”‚                     â”‚   Server     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                    â”‚
       â”‚ HTTP (File Upload)          HTTP (Chat)
       â”‚                                    â”‚
       â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Flask Backend (Python)              â”‚
â”‚  â€¢ Document processing & text extraction         â”‚
â”‚  â€¢ Text chunking with overlap                    â”‚
â”‚  â€¢ Vector embeddings (Sentence Transformers)     â”‚
â”‚  â€¢ Similarity search (FAISS)                     â”‚
â”‚  â€¢ AI response generation (Ollama)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Frontend**
- HTML5, CSS3, JavaScript (Vanilla)
- WebSocket API for real-time communication
- Drag-and-drop file upload
- Responsive design

**Backend**
- **Node.js** + Express - WebSocket server for real-time messaging
- **Flask** (Python) - REST API for document processing and AI
- **FAISS** - Facebook AI Similarity Search for vector operations
- **Sentence Transformers** - Text embedding generation (all-MiniLM-L6-v2)
- **Ollama** - Local LLM runtime (phi4-mini:3.8b)

**Document Processing**
- **PyPDF2** - PDF text extraction
- **python-docx** - Microsoft Word document parsing
- Plain text file support
- Smart chunking algorithm with overlap

**Data Persistence**
- FAISS index files for vector storage
- Pickle serialization for metadata
- SQLite for conversation history

---

## âœ¨ Key Features Implemented

### 1. Document Processing Pipeline
- **Multi-format support**: PDF, DOCX, TXT files
- **Smart chunking**: 500-character chunks with 50-character overlap for context preservation
- **Automatic indexing**: Documents are immediately available for querying after upload
- **Metadata tracking**: Source file, chunk IDs, and lengths stored with vectors
- **File listing API**: Retrieve all uploaded files with chunk counts

### 2. Retrieval-Augmented Generation (RAG)
- **Semantic search**: Uses cosine similarity to find relevant document chunks
- **Selective filtering**: Query specific documents by filename selection
- **Context injection**: Top 3 most relevant chunks provided to LLM with source files
- **Source attribution**: Responses show which file each context chunk came from
- **Fallback handling**: Gracefully handles queries without relevant context

### 3. User Interface
- **Real-time chat**: WebSocket-based instant messaging
- **Interactive file list**: Checkbox-based document selection with chunk counts
- **Select all/none buttons**: Quick file selection controls
- **File management**: Upload, view statistics, clear documents
- **Visual feedback**: Loading states, upload progress, success/error notifications
- **Statistics dashboard**: Real-time chunk count and collection status
- **Responsive design**: Works on desktop and mobile devices

### 4. API Endpoints
- `POST /upload` - Document upload and processing
- `POST /chat` - AI chat with RAG support and optional file filtering
- `GET /stats` - Vector store statistics
- `GET /files` - List all uploaded files with metadata
- `POST /clear` - Clear all documents

---

## ğŸ”§ Technical Challenges Solved

### Challenge 1: Multiprocessing Semaphore Leaks
**Problem**: Python backend crashed after file uploads due to leaked semaphore objects from sentence-transformers' parallel tokenization conflicting with Flask's debug mode reloader.

**Solution**: 
- Disabled tokenizer parallelism via environment variables (`TOKENIZERS_PARALLELISM=false`)
- Configured single-threaded OpenMP operations (`OMP_NUM_THREADS=1`)
- Modified SentenceTransformer initialization to explicitly use CPU device
- Added model warmup to prevent initialization issues
- Disabled progress bars that used multiprocessing

**Result**: Stable file upload processing with zero crashes

### Challenge 2: Library Compatibility Issues
**Problem**: Initial ChromaDB implementation required onnxruntime which was incompatible with macOS 12.x (Monterey) and Python 3.13.

**Solution**: 
- Migrated from ChromaDB to FAISS (Facebook AI Similarity Search)
- Implemented custom metadata storage using pickle serialization
- Maintained identical API interface for seamless transition
- Reduced dependencies and installation complexity

**Benefits**:
- Cross-platform compatibility
- Faster installation and startup
- Lower memory footprint
- Simpler codebase

### Challenge 3: Context Window Management
**Problem**: Large documents could overwhelm LLM context window and reduce response quality.

**Solution**:
- Implemented smart chunking with configurable size and overlap
- Limited retrieval to top 3 most relevant chunks (configurable)
- Added context length tracking and validation
- Optimized chunk size for balance between context and relevance

### Challenge 4: Multi-Document Query Filtering
**Problem**: Users needed ability to query specific documents rather than entire collection.

**Solution**:
- Added file metadata tracking in vector store
- Implemented efficient filtering in search algorithm
- Created interactive UI with checkboxes for file selection
- Enhanced WebSocket protocol to support structured messages with file selection
- Added source attribution in AI responses showing which file provided each context

**Benefits**:
- Focused queries on relevant documents
- Better context precision for domain-specific questions
- Improved user control over RAG behavior

---

## ğŸ“Š Project Metrics

**Code Base**
- **Python Backend**: ~450 lines across 3 modules
- **Node.js Server**: ~100 lines
- **Frontend**: ~400 lines (HTML/CSS/JS)
- **Documentation**: Comprehensive README, setup guides, troubleshooting

**Performance**
- Document upload: < 1 second for typical documents
- Embedding generation: ~0.1-0.5 seconds per chunk
- Query response time: ~0.5-1 second (retrieval + generation)
- Memory usage: ~500MB (includes model in memory)

**Features**
- 5 REST API endpoints
- 3 document formats supported
- Real-time WebSocket communication with structured messages
- Vector similarity search with file filtering
- Interactive document selection UI
- Local AI inference

---

## ğŸš€ Setup & Deployment

### Quick Start
```bash
# 1. Install dependencies
./install.sh

# 2. Start backend (Terminal 1)
./start-backend.sh

# 3. Start frontend (Terminal 2)
./start-frontend.sh

# 4. Access application
open http://localhost:3000
```

### System Requirements
- Python 3.8+ (tested with 3.13)
- Node.js 16+ 
- Ollama with phi4-mini:3.8b model
- 2GB RAM minimum, 4GB recommended

### Configuration Files
- **Automated setup script** (`install.sh`)
- **Startup scripts** for backend and frontend
- **Requirements.txt** with pinned versions
- **Comprehensive .gitignore** for clean repository

---

## ğŸ“š Documentation Provided

1. **README.md** - Quick start guide and overview
2. **RAG_SETUP.md** - Detailed setup instructions and architecture
3. **IMPORTANT_CHANGES.md** - Technical decisions and migration notes
4. **MULTIPROCESSING_FIX.md** - Detailed solution documentation
5. **PROJECT_SUMMARY.md** - This document

---

## ğŸ’¡ Skills Demonstrated

### Technical Skills
- **Full-Stack Development**: Python, JavaScript, Node.js
- **AI/ML Integration**: Embeddings, vector search, LLM orchestration
- **API Design**: RESTful APIs, WebSocket communication
- **Database Management**: Vector databases, SQLite
- **Problem Solving**: Debug complex multiprocessing issues
- **System Architecture**: Design scalable RAG pipeline

### Software Engineering Practices
- **Clean Code**: Modular design, separation of concerns
- **Documentation**: Comprehensive technical documentation
- **Error Handling**: Graceful failures and user feedback
- **Version Control**: Git with organized commit history
- **Configuration Management**: Environment variables, .gitignore

### Domain Knowledge
- **Natural Language Processing**: Text chunking, embeddings, semantic search
- **RAG Architecture**: Context retrieval and augmentation
- **LLM Integration**: Prompt engineering, context management
- **Vector Operations**: Similarity search, indexing strategies

---

## ğŸ”® Future Enhancements

- User authentication and multi-user support
- Persistent conversation history with search
- Support for more document formats (Excel, CSV, Markdown)
- Advanced chunking strategies (semantic, paragraph-aware)
- Multiple embedding model support
- API rate limiting and caching
- Production deployment with Docker
- Batch document processing
- Document version control
- Delete individual files (not just clear all)
- Rename uploaded files
- Document preview functionality

---

## ğŸ“ Contact & Demo

**Project Repository**: Available upon request  
**Live Demo**: Can be scheduled for walkthrough  
**Technical Discussion**: Happy to explain any implementation details

---

## âœ… Project Status

**Status**: âœ… Fully Functional with File Selection Feature  
**Deployment Ready**: Development version complete  
**Last Updated**: November 6, 2025  
**Latest Feature**: Selective file querying with interactive UI
**Test Coverage**: Manual testing completed  
**Production Readiness**: 80% (needs auth, rate limiting for production)

---

*This project demonstrates practical application of modern AI/ML technologies, full-stack development capabilities, and problem-solving skills in real-world scenarios. Built with attention to code quality, documentation, and user experience.*

