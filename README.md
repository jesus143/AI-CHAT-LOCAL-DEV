**Create you AI chat locally**

**html + css** -> frontend <br>
**Nodejs** -> realtime chat <br>
**Flask** -> backend <br>
**ollama** -> the runtime (the engine that loads and runs models). <br>
**phi4-mini:3.8b** -> the model you are running inside Ollama. <br>


Python Service <br> 
macos <br>
python3 -m venv venv <br>
source venv/bin/activate <br>
cd AI-CHAT-LOCAL-DEV/python <br>
run php install flask <br>
python app.py  <br>
 * Running on http://127.0.0.1:5000 <br>



Nodejs <br>
cd AI-CHAT-LOCAL-DEV <br>
npm run dev <br>
ðŸš€ Server running at http://localhost:3000 <br>



Visit Browser on <br>
http://localhost:3000 <br>



